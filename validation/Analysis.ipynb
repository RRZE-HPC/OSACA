{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "  function code_toggle() {\n",
       "    if (code_shown){\n",
       "      $('div.input').hide('500');\n",
       "      $('#toggleButton').val('Show Code')\n",
       "    } else {\n",
       "      $('div.input').show('500');\n",
       "      $('#toggleButton').val('Hide Code')\n",
       "    }\n",
       "    code_shown = !code_shown\n",
       "  }\n",
       "\n",
       "  $( document ).ready(function(){\n",
       "    code_shown=false;\n",
       "    $('div.input').hide()\n",
       "  });\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Validation of OSACA\n",
    "OSACA predicts the minimal (throughput and loop-carried dependency latency) and maximum (critical path latency) instruction execution time for assembly loop bodies, under the assumption that all loads and stores are served by the first level cache.\n",
    "\n",
    "In this validation study OSACA is compared to measured runtimes of assembly blocks in a large dataset. The prediction accuracy is also compared to three other tools with the same goal im mind: IACA, LLVM-MCA and Ithemal.\n",
    "\n",
    "## Validation Dataset\n",
    "The validation dataset is created by compiling several kernels with diffent compilers and optimization flags. Kernels represent the domain of numerical codes and all use double precision floating-point arithmetics. The original high-level C implementations are found in the `validation/kernels` directory and include:\n",
    " * Simple load-store heavy kernels: `add`, `copy`, `store`, `striad`, `triad` and `update`\n",
    " * Stencils: `2d-5pt`, `3d-7pt`, `3d-27pt` and `3d-r3-11pt`\n",
    " * Dependency chain: `gs-2d-5pt` and `sumreduction`\n",
    " * Slow arithmetic: `pi` (divide)\n",
    "\n",
    "All these kernels are compiled with `gcc`, `clang` and `icc` (where applicable) and for each compiler with `-O1`, `-O2`, `-O3` and `-Ofast`, which results in 6-9 variantes per kernel. Additional kernels, compiler and flags can be configured in the `build_and_run.py` script.\n",
    "\n",
    "## Runtimes and Ground Truth Measurements\n",
    "\n",
    "Reported cycle runtimes in this notebook –unless otherwise noted– are scaled to high-level code iterations to make them comparable. Raw outputs by the tools will refer to assembly block iterations. Since all provided kernels are double precision based, the `pointer_increment` divided by `8` gives the used scaling factor.\n",
    "\n",
    "Measured runtimes are gained by traversing 8 to 1024 elements (for a total of 25 million iterations) and selecting the smallest measured runtime out of all executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:264: DeprecationWarning: 'U' mode is deprecated\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from pprint import pprint, pformat\n",
    "from itertools import product\n",
    "import warnings\n",
    "from functools import reduce\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib as mpl\n",
    "    import matplotlib.pyplot as plt\n",
    "    from IPython.display import display, HTML\n",
    "\n",
    "def boxprint(*args):\n",
    "    for arg in args:\n",
    "        display(HTML('<pre style=\"white-space: pre !important;\">{}</pre>'.format(arg)))\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Load data from build/ directory. The data can either be generated using build_and_run.py or downloaded from a \n",
    "# OSACA release from github. Look for releases including validation data archives.\n",
    "data = []\n",
    "for p in Path('build/').glob('*/data.pkl'):\n",
    "    data += pickle.loads(p.read_bytes())\n",
    "df = pd.DataFrame(data)\n",
    "archs = ['IVB', 'SKX', 'ZEN', 'ZEN2', 'TX2', 'A64FX']\n",
    "archs += [a for a in sorted(df.arch.unique()) if a not in archs]\n",
    "models = ['IACA', 'Ithemal', 'LLVM-MCA', 'OSACA']\n",
    "kernels = sorted(df.kernel.unique())\n",
    "compilers = sorted(df.compiler.unique())\n",
    "cflags_names = sorted(df.cflags_name.unique())\n",
    "\n",
    "# Add more data\n",
    "df['worst_runtime'] = df.allruns.map(lambda ar: max([r[2] for r in ar]), na_action='ignore')\n",
    "for pred in models:\n",
    "    df[pred+'_err'] = df['best_runtime'] - df[pred+'_prediction']\n",
    "    df[pred+'_relerr'] = df[pred+'_err']/df['best_runtime']\n",
    "    df[pred+'_tperr'] = df['best_runtime'] - df[pred+'_throughput']\n",
    "    df[pred+'_reltperr'] = df[pred+'_tperr']/df['best_runtime']\n",
    "df['instruction count'] = df.OSACA_raw.map(lambda r: len([l for l in r['analyzed kernel'] if l.get('instruction')]))\n",
    "\n",
    "# Clean up\n",
    "# TODO do this in pickle files!\n",
    "del df['IACA_scaled_max'], df['OSACA_scaled_max'], df['LLVM-MCA_scaled_max']\n",
    "del df['IACA_scaled'], df['OSACA_scaled'], df['LLVM-MCA_scaled']\n",
    "\n",
    "# Make indexed representation\n",
    "df_idx = df.set_index(['arch', 'compiler', 'cflags_name', 'kernel'], verify_integrity=True)\n",
    "df_idx.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for missing instructions in all OSACA analyses\n",
    "for idx, raw in df.OSACA_raw.items():\n",
    "    if not isinstance(raw, dict): continue\n",
    "    count = 0\n",
    "    for l in raw['analyzed kernel']:\n",
    "        if 'tp_unknown' in l['flags']:\n",
    "            print(df['arch'][idx]+'/'+df['compiler'][idx]+'/'+df['cflags_name'][idx]+'/'+df['kernel'][idx]+'.marked.s', l['line'])\n",
    "            count += 1\n",
    "if not count:\n",
    "    print(\"\\u2705 All instructions in test dataset are defined in OSACA microarchitecture databaseses.\")\n",
    "else:\n",
    "    print(count,\n",
    "          \"instructions were not found in OSACA microarchitecture databaseses. \"\n",
    "          \"This may lead to unnecessary underpredictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics on Dataset and Measurements\n",
    "The following histogram/bar plots give an overview on of the wide range of kernels found in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, figsize=(15, 15))\n",
    "df['best_runtime_asm'] = (df.best_runtime * (df.pointer_increment // 8))\n",
    "axs[0].hist(\n",
    "    [df.best_runtime_asm[df.arch == a] for a in archs],\n",
    "    bins=range(0,90),\n",
    "    label=archs,\n",
    "    stacked=True,\n",
    ")\n",
    "axs[0].legend()\n",
    "axs[0].set_ylabel('stacked count per (1-cycle wide) bin')\n",
    "axs[0].set_title('Histogram of measured cycle runtimes per assembly block iteration')\n",
    "\n",
    "(df.pointer_increment // 8).groupby(df.arch).value_counts().sort_index().unstack('arch').plot(\n",
    "    kind='bar',\n",
    "    ax=axs[1],\n",
    "    stacked=True,\n",
    "    legend=True\n",
    ")\n",
    "axs[1].set_xlabel('')\n",
    "axs[1].set_ylabel('stacked count')\n",
    "axs[1].set_title(\"High-level iterations per assembly block\")\n",
    "\n",
    "axs[2].hist(\n",
    "    [df['instruction count'][df.arch == a] for a in archs],\n",
    "    bins=range(0,100),\n",
    "    label=archs,\n",
    "    stacked=True,\n",
    ")\n",
    "axs[0].legend()\n",
    "axs[2].set_ylabel('stacked count per bin')\n",
    "axs[2].set_title(\"Histogram of instruction count per assembly block\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_forms = reduce(\n",
    "    set.union,\n",
    "    df.OSACA_raw.map(lambda r: {l['instruction']+repr([','.join(o.keys()) for o in l['operands']])\n",
    "                                for l in r['analyzed kernel']\n",
    "                                if l['instruction']}))\n",
    "print(\"A total of\",\n",
    "      len(instruction_forms),\n",
    "      \"different instruction forms (different in name or operand type) found in dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot show the convergence of the kernel runtimes towards a minimum plataeu, which is used as ground truth and to be predicted. Noise, toxic loop lengths and runtime increases towards the right are to be expected. Double click on plot to enlarge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kernels = sorted(df.kernel.unique())\n",
    "archs = sorted(df.arch.unique())\n",
    "compiler_flags_prod = list(product(sorted(df.compiler.unique()), sorted(df.cflags_name.unique())))\n",
    "linestyle_dict = {'Ofast': '-', 'O3': '--', 'O2': '-.', 'O1': ':'}\n",
    "\n",
    "kernels = sorted(df.kernel.unique())\n",
    "fig, axs = plt.subplots(len(kernels), len(archs), figsize=(3.5*len(archs)+1,2*len(kernels)+1),\n",
    "                        sharex=True, dpi=250, squeeze=False)\n",
    "colors = list(map(list, plt.cm.get_cmap('tab20').colors))\n",
    "handles = []\n",
    "for ki, kernel in enumerate(kernels):\n",
    "    for ai, arch in enumerate(archs):\n",
    "        for idx, row in df.query('kernel == @kernel and arch == @arch').iterrows():\n",
    "            if type(row.allruns) is list:\n",
    "                color_index = compiler_flags_prod.index((row.compiler, row.cflags_name))\n",
    "                handles = axs[ki,ai].plot(\n",
    "                    [r[0] for r in row.allruns][:1000], [r[2] for r in row.allruns][:1000],\n",
    "                    color=colors[color_index],\n",
    "                    linestyle=linestyle_dict[row.cflags_name],\n",
    "                    label=str((row.compiler, row.cflags_name)))\n",
    "            \n",
    "            if ki == 0:\n",
    "                axs[ki,ai].set_title(arch)\n",
    "            elif ki == len(kernels) - 1:\n",
    "                axs[ki,ai].set_xlabel(\"8-byte elements\")\n",
    "            if ai == 0:\n",
    "                axs[ki,ai].set_ylabel(kernel+\"\\ncycle per it.\")\n",
    "            axs[ki,ai].grid(True)\n",
    "fig.legend(handles=axs[0,1].lines, loc=\"lower center\", bbox_to_anchor=(0.5, 0.07), ncol=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Statistics on Predictions\n",
    "Predictions are evaluated with a relative error metric: $\\mathrm{rel. error} = (T_\\mathrm{measured} - T_\\mathrm{predicted})/T_\\mathrm{measured}$. Optimally this is to be zero. If the relative error is negative the model has *underpredicted* and failed being a lower bound model, because it predicted a faster runtime than was measured. If the realtive error is positive, the model *overpredicted*, then it is inaccurate and predicted a faster runtime than was measured. A relative prediction error larger than one would indicate a negative absolue runtime prediction and is therefore not possible.\n",
    "\n",
    "The following table and plot give an overview of the models relative errors in regard to the used dataset. The number of tests depend on the architecture (`icc` does not compile for `aarch64`) and model (IACA and Ithemal do not support non-Intel architectures and Ithemal fails on AVX512 instructions) used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"{:>8}  {:>5}  {:>8}  {:>15}  {:>15}  {:>15}\".format(\"model\", \"tests\", \"+20% fit\", \"avg(relerr < 0)\", \"avg(relerr > 0)\", \"min(relerr)\"))\n",
    "print(\"{:>8}  {:>5}  {:>8}  {:>15}  {:>15}  {:>15}\".format(\"\", \"\", \"+10% fit\", \"sum(relerr < 0)\", \"sum(relerr > 0)\", \"max(relerr)\"))\n",
    "print(\" \"*25+\"  not lower bound         accuracy\\n\")\n",
    "for m in models:\n",
    "    line = \"{:>8}  \".format(m)\n",
    "    line2 = \" \"*10\n",
    "    mr = df[df[m+'_relerr'].notna()]\n",
    "    # Test count\n",
    "    line += \"{:>5}  \".format(len(mr))\n",
    "    line2 += \" \"*7\n",
    "    # Fit\n",
    "    line += \"{:>8}  \".format(len(mr.query(\"0 < `\"+m+\"_relerr` < 0.2\")))\n",
    "    line2 += \"{:>8}  \".format(len(mr.query(\"0 < `\"+m+\"_relerr` < 0.1\")))\n",
    "    # Sum relative error < 0 (bad)\n",
    "    relerr_neg = mr[mr[m+'_relerr'] < 0][m+'_relerr']\n",
    "    if len(relerr_neg):\n",
    "        line += \"{:>15.3f}  \".format(sum(relerr_neg)/len(relerr_neg))\n",
    "    else:\n",
    "        line += \" \"*14+\"-  \"\n",
    "    line2 += \"{:>15.3f}  \".format(sum(relerr_neg))\n",
    "    # Sum relative error > 0 (could be better)\n",
    "    relerr_pos = mr[mr[m+'_relerr'] > 0][m+'_relerr']\n",
    "    if len(relerr_neg):\n",
    "        line += \"{:>15.3f}  \".format(sum(relerr_pos)/len(relerr_pos))\n",
    "    else:\n",
    "        line += \" \"*14+\"-  \"\n",
    "    line2 += \"{:>15.3f}  \".format(sum(relerr_pos))\n",
    "    if not mr[m+'_relerr'].empty:\n",
    "        line += \"{:>15.3f}  \".format(min(mr[m+'_relerr']))\n",
    "        line2 += \"{:>15.3f}  \".format(max(mr[m+'_relerr']))\n",
    "    print(line)\n",
    "    print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_bin_edges = list(np.arange(-1.0, 1.1, 0.1))\n",
    "fig, axs = plt.subplots(1, len(models), figsize=(15, 5), sharey='all', sharex='all')\n",
    "for i, m in enumerate(models):\n",
    "    h = axs[i].hist(\n",
    "        [df[df.arch == a][m+'_relerr'] for a in archs],\n",
    "        bins=[-100.0]+inner_bin_edges,\n",
    "        stacked=True,\n",
    "        label=archs,\n",
    "    )\n",
    "    axs[i].set_xticks([min(inner_bin_edges)-0.2] + inner_bin_edges[::5])\n",
    "    axs[i].set_xticks(inner_bin_edges, minor=True)\n",
    "    axs[i].set_xlim(min(inner_bin_edges)-0.2, max(inner_bin_edges))\n",
    "    axs[i].axvline(0, color='red', linestyle='--')\n",
    "    if i == 0:\n",
    "        axs[i].set_ylabel('stacked counts per bin')\n",
    "    axs[i].set_xlabel('relative prediction error\\n((measured - pred.)/measured)')\n",
    "    axs[i].set_xticklabels(['<'] + [f\"{e:.1f}\" for e in inner_bin_edges[::5]])\n",
    "    axs[i].set_title(m)\n",
    "    axs[i].grid(True)\n",
    "# < 0 -> underprediction (predicted too slow), lower bound model failed\n",
    "# > 0 -> overprediction (predicted too fast), higher means less accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* IACA shows the best average overprediction with about 18%. It does underpredict, but rearly. Almost two-thirds of the applicable tests are predicted within 20% of the measured runtime.\n",
    "* Ithemal underpredicts most of the time and gives a reasonable prediction in in less than one-tenth of the sucessfully run test cases.\n",
    "* LLVM-MCA underpredicts quite often, none the less, most overpredictions are within 0 and +50% of the measured runtime, 35% on average.\n",
    "* OSACA seldomly under predicts (details on this follow below), and 44% of all test cases show a relative error of 20% or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"{:>8}  {:>5}  {:>8}  {:>15}  {:>15}  {:>15}\".format(\"µarch\", \"tests\", \"+20% fit\", \"avg(relerr < 0)\", \"avg(relerr > 0)\", \"min(relerr)\"))\n",
    "print(\"{:>8}  {:>5}  {:>8}  {:>15}  {:>15}  {:>15}\\n\".format(\"\", \"\", \"+10% fit\", \"sum(relerr < 0)\", \"sum(relerr > 0)\", \"max(relerr)\"))\n",
    "for a in archs:\n",
    "    m = 'OSACA'\n",
    "    line = \"{:>8}  \".format(a)\n",
    "    line2 = \" \"*10\n",
    "    mr = df[df[m+'_relerr'].notna()][df.arch == a]\n",
    "    # Test count\n",
    "    line += \"{:>5}  \".format(len(mr))\n",
    "    line2 += \" \"*7\n",
    "    # Fit\n",
    "    line += \"{:>8}  \".format(len(mr.query(\"0 < `\"+m+\"_relerr` < 0.2\")))\n",
    "    line2 += \"{:>8}  \".format(len(mr.query(\"0 < `\"+m+\"_relerr` < 0.1\")))\n",
    "    # Sum relative error < 0 (bad)\n",
    "    relerr_neg = mr[mr[m+'_relerr'] < 0][m+'_relerr']\n",
    "    if len(relerr_neg):\n",
    "        line += \"{:>15.3f}  \".format(sum(relerr_neg)/len(relerr_neg))\n",
    "    else:\n",
    "        line += \" \"*14+\"-  \"\n",
    "    line2 += \"{:>15.3f}  \".format(sum(relerr_neg))\n",
    "    # Sum relative error > 0 (could be better)\n",
    "    relerr_pos = mr[mr[m+'_relerr'] > 0][m+'_relerr']\n",
    "    if len(relerr_neg):\n",
    "        line += \"{:>15.3f}  \".format(sum(relerr_pos)/len(relerr_pos))\n",
    "    else:\n",
    "        line += \" \"*14+\"-  \"\n",
    "    line2 += \"{:>15.3f}  \".format(sum(relerr_pos))\n",
    "    if not mr[m+'_relerr'].empty:\n",
    "        line += \"{:>15.3f}  \".format(min(mr[m+'_relerr']))\n",
    "        line2 += \"{:>15.3f}  \".format(max(mr[m+'_relerr']))\n",
    "    print(line)\n",
    "    print(line2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This breakdown of OSACA predictions by archiecture reveals that IVB and ZEN are best predicted and TX2 and A64FX worst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intel Ivy Bridge (IVB) and Skylake X (SKX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inner_bin_edges = list(np.arange(-1.0, 1.1, 0.1))\n",
    "colors = list(map(list, plt.cm.get_cmap('tab20').colors))\n",
    "fig, axs = plt.subplots(2, len(models), figsize=(20, 9), sharey='all', sharex='all')\n",
    "for j, a in enumerate(['IVB', 'SKX']):\n",
    "    for i, m in enumerate(models):\n",
    "        h = axs[j,i].hist(\n",
    "            [df[df.arch == a][m+'_relerr'][df.kernel == k] for k in kernels],\n",
    "            bins=[-100.0]+inner_bin_edges,\n",
    "            stacked=True,\n",
    "            label=kernels,\n",
    "            color=colors[:len(kernels)]\n",
    "        )\n",
    "        axs[j,i].set_xticks([min(inner_bin_edges)-0.2] + inner_bin_edges[::5])\n",
    "        axs[j,i].set_xticks(inner_bin_edges, minor=True)\n",
    "        axs[j,i].set_xlim(min(inner_bin_edges)-0.2, max(inner_bin_edges))\n",
    "        axs[j,i].axvline(0, color='red', linestyle='--')\n",
    "        if i == 0:\n",
    "            axs[j,i].set_ylabel('stacked counts per bin')\n",
    "            axs[j,i].set_yticks(range(0, 81, 10))\n",
    "        if j == axs.shape[0]-1:\n",
    "            axs[j,i].set_xlabel('relative prediction error\\n((measured - pred.)/measured)')\n",
    "            axs[j,i].set_xticklabels(['<'] + [f\"{e:.1f}\" for e in inner_bin_edges[::5]])\n",
    "        axs[j,i].set_title(m+\" for \"+a)\n",
    "        axs[j,i].grid(True)\n",
    "fig.legend(h[2], kernels, ncol=7, loc=\"lower center\", bbox_to_anchor=(.5, -0.02));\n",
    "# < 0 -> underprediction (predicted too slow), lower bound model failed\n",
    "# > 0 -> overprediction (predicted too fast), higher means less accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions on Skylake are challenging to both LLVM-MCA and OSACA, while IACA gives good results. Looking at individual IACA prediction reports quickly reveals that IACA assumes many of the more-than-20%-off OSACA predictions to be bound by a frontend bottleneck. Since frontend bottlenecks are not modled by OSACA, this is to be expected. While this does not explain the whole discrepancy, some can be accounted to this, as shown in the following plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "inner_bin_edges = list(np.arange(-0.1, 1.1, 0.1))\n",
    "df['IACA_frontend'] = df.IACA_raw.map(lambda r: \"FrontEnd\" in r['output'], na_action='ignore')\n",
    "df.IACA_frontend.fillna(False, inplace=True)\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "axs[0].hist(\n",
    "    [df[df.arch == \"SKX\"]['IACA_relerr'][df.IACA_frontend == b] for b in [True, False]],\n",
    "    bins=[-100.0]+inner_bin_edges,\n",
    "    stacked=True,\n",
    ")\n",
    "axs[0].set_ylabel(\"stacked counts per bin (10% width)\")\n",
    "axs[0].set_title(\"IACA\")\n",
    "\n",
    "axs[1].hist(\n",
    "    [df[df.arch == \"SKX\"]['LLVM-MCA_relerr'][df.IACA_frontend == b] for b in [True, False]],\n",
    "    bins=[-100.0]+inner_bin_edges,\n",
    "    stacked=True,\n",
    ")\n",
    "axs[1].set_title(\"LLVM-MCA\")\n",
    "\n",
    "\n",
    "h = axs[2].hist(\n",
    "    [df[df.arch == \"SKX\"]['OSACA_relerr'][df.IACA_frontend == b] for b in [True, False]],\n",
    "    bins=[-100.0]+inner_bin_edges,\n",
    "    stacked=True,\n",
    ")\n",
    "axs[2].set_title(\"OSACA\")\n",
    "\n",
    "for i in range(3):\n",
    "    axs[i].set_xticks([min(inner_bin_edges)-0.2] + inner_bin_edges[::5])\n",
    "    axs[i].set_xticks(inner_bin_edges, minor=True)\n",
    "    axs[i].set_xticklabels(['<'] + [f\"{e:.1f}\" for e in inner_bin_edges[::5]])\n",
    "    axs[i].set_xlim(min(inner_bin_edges)-0.2, max(inner_bin_edges))\n",
    "    axs[i].set_xlabel(\"relative prediction error\")\n",
    "fig.legend(h[2], [\"frontend\", \"other\"], title=\"IACA bottleneck\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AMD Zen and Zen2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "inner_bin_edges = list(np.arange(-1.0, 1.1, 0.1))\n",
    "colors = list(map(list, plt.cm.get_cmap('tab20').colors))\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10,10), sharey='all', sharex='all')\n",
    "for j, a in enumerate(['ZEN', 'ZEN2']):\n",
    "    for i, m in enumerate(['LLVM-MCA','OSACA']):\n",
    "        h = axs[j,i].hist(\n",
    "            [df[df.arch == a][m+'_relerr'][df.kernel == k] for k in kernels],\n",
    "            bins=[-100.0]+inner_bin_edges+[100.0],\n",
    "            stacked=True,\n",
    "            label=kernels,\n",
    "            color=colors[:len(kernels)],\n",
    "        )\n",
    "        axs[j,i].set_xticks([min(inner_bin_edges)-0.2] + inner_bin_edges[::5])\n",
    "        axs[j,i].set_xticks(inner_bin_edges, minor=True)\n",
    "        axs[j,i].set_xlim(min(inner_bin_edges)-0.2, max(inner_bin_edges))\n",
    "        axs[j,i].axvline(0, color='red', linestyle='--')\n",
    "        if i == 0:\n",
    "            axs[j,i].set_ylabel('stacked counts')\n",
    "        if j == axs.shape[0]-1:\n",
    "            axs[j,i].set_xlabel('relative prediction error\\n((measured - pred.)/measured)')\n",
    "            axs[j,i].set_xticklabels(['<'] + [f\"{e:.1f}\" for e in inner_bin_edges[::5]])\n",
    "        axs[j,i].set_title(m+\" for \"+a)\n",
    "        axs[j,i].grid(True)\n",
    "fig.legend(h[2], kernels, ncol=7, loc=\"lower center\", bbox_to_anchor=(.5, -0.01));\n",
    "# < 0 -> underprediction (predicted too slow), lower bound model failed\n",
    "# > 0 -> overprediction (predicted too fast), higher means less accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARM-based Fujitsu A64FX and Cavium ThunderX2 (TX2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "inner_bin_edges = list(np.arange(-1.0, 1.1, 0.1))\n",
    "colors = list(map(list, plt.cm.get_cmap('tab20').colors))\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10,10), sharey='all', sharex='all')\n",
    "for j, a in enumerate(['A64FX', 'TX2']):\n",
    "    for i, m in enumerate(['LLVM-MCA','OSACA']):\n",
    "        h = axs[j,i].hist(\n",
    "            [df[df.arch == a][m+'_relerr'][df.kernel == k] for k in kernels],\n",
    "            bins=[-100.0]+inner_bin_edges+[100.0],\n",
    "            stacked=True,\n",
    "            label=kernels,\n",
    "            color=colors[:len(kernels)],\n",
    "        )\n",
    "        axs[j,i].set_xticks([min(inner_bin_edges)-0.2] + inner_bin_edges[::5])\n",
    "        axs[j,i].set_xticks(inner_bin_edges, minor=True)\n",
    "        axs[j,i].set_xlim(min(inner_bin_edges)-0.2, max(inner_bin_edges))\n",
    "        axs[j,i].axvline(0, color='red', linestyle='--')\n",
    "        if i == 0:\n",
    "            axs[j,i].set_ylabel('stacked counts per bin')\n",
    "        if j == axs.shape[0]-1:\n",
    "            axs[j,i].set_xlabel('relative prediction error\\n((measured - pred.)/measured)')\n",
    "            axs[j,i].set_xticklabels(['<'] + [f\"{e:.1f}\" for e in inner_bin_edges[::5]])\n",
    "        axs[j,i].set_title(m+\" for \"+a)\n",
    "        axs[j,i].grid(True)\n",
    "fig.legend(h[2], kernels, ncol=7, loc=\"lower center\", bbox_to_anchor=(.5, -0.01));\n",
    "# < 0 -> underprediction (predicted too slow), lower bound model failed\n",
    "# > 0 -> overprediction (predicted too fast), higher means less accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "pi_bins = [8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "colors = list(map(list, plt.cm.get_cmap('tab10').colors))\n",
    "fig, axs = plt.subplots(len(archs), len([m for m in models if m != \"Ithemal\"]),\n",
    "                        figsize=(20,1+len(archs)*3), sharey='row', sharex='all')\n",
    "for j, a in enumerate(archs):\n",
    "    for i, m in enumerate([m for m in models if m != \"Ithemal\"]):\n",
    "        h = axs[j,i].hist(\n",
    "            [df[df.arch == a].query('pointer_increment == @pi')[m+'_relerr'] for pi in pi_bins],\n",
    "            bins=np.arange(-2.0, 1.1, 0.1),\n",
    "            stacked=True,\n",
    "            label=map(str, pi_bins),\n",
    "            color=colors[:len(pi_bins)],\n",
    "        )\n",
    "        axs[j,i].axvline(0, color='red', linestyle='--')\n",
    "        if df[df.arch == a][m+'_relerr'].isnull().all():\n",
    "            axs[j,i].text(0.5, 0.5, \"not\\nsupported\",\n",
    "                          transform=axs[j,i].transAxes, horizontalalignment='center')\n",
    "        if i == 0:\n",
    "            axs[j,i].set_ylabel('stacked counts per bin')\n",
    "        if j == len(archs)-1:\n",
    "            axs[j,i].set_xlabel('relative prediction error\\n((measured - pred.)/measured)')\n",
    "        axs[j,i].set_title(m+\" for \"+a)\n",
    "        axs[j,i].grid(True)\n",
    "fig.legend(h[2], pi_bins, ncol=10, loc=\"lower center\", bbox_to_anchor=(.5, 0.05), title=\"Pointer Increment\");\n",
    "# < 0 -> underprediction (predicted too slow), lower bound model failed\n",
    "# > 0 -> overprediction (predicted too fast), higher means less accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underpredictions by OSACA\n",
    "The following table shows all underpredicitons, which contradict the light-speed/lower-bound model paradim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Runtime less than TP or LCD, this violates the model assumption:\n",
    "df_idx.query(\"best_runtime < OSACA_throughput or best_runtime < OSACA_lcd\").sort_values('OSACA_relerr')[\n",
    "    ['best_runtime', 'OSACA_prediction', 'OSACA_relerr']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two underpredicted kernels on A64FX are the only ones which show an error \"greater\" than 0.5%. Both kernels are very unbalanced port-wise, which could be avoided by better scheduling. Usually optimal scheduling is assumed by OSACA, but naive scheduling has proven to be better suited on A64FX, at the cost of these rare underpredictions.\n",
    "\n",
    "All other kernels show a very small underprediction, which may be related to underreprting of the processor frequency during measurement or even rounding errors (e.g., 2.9997 vs 3.000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Runtime more than 110% of CP, this may be a model error:\n",
    "df_idx.query(\"best_runtime > OSACA_cp*1.1\")[\n",
    "    ['IACA_prediction', 'LLVM-MCA_prediction',\n",
    "     'OSACA_throughput', 'OSACA_lcd', 'OSACA_cp',\n",
    "     'LLVM-MCA_cp',\n",
    "     'best_runtime', 'best_length', 'pointer_increment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# worst predictions\n",
    "df_idx.sort_values('OSACA_relerr', ascending=False)[\n",
    "    ['IACA_prediction', 'LLVM-MCA_prediction',\n",
    "     'OSACA_throughput', 'OSACA_lcd', 'OSACA_cp', 'OSACA_relerr',\n",
    "     'best_runtime', 'best_length', 'pointer_increment']][:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "df_idx['relerrdiff_IO'] = df_idx.IACA_relerr - df_idx.OSACA_relerr\n",
    "df_idx['IACA_Bottleneck'] = df_idx.IACA_raw.map(lambda r: re.search(r'Throughput Bottleneck: (.*)\\n', r['output']).group(1), na_action='ignore')\n",
    "df_idx[df_idx.relerrdiff_IO < 0].sort_values(\"relerrdiff_IO\")[\n",
    "    ['relerrdiff_IO', 'IACA_prediction', 'IACA_Bottleneck', 'OSACA_throughput', 'OSACA_lcd', 'OSACA_cp',\n",
    "     'best_runtime', 'best_length', 'pointer_increment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "df_idx['relerrdiff_LO'] = df_idx['LLVM-MCA_relerr'] - df_idx.OSACA_relerr\n",
    "df_idx[df_idx['LLVM-MCA_relerr'] > 0][df_idx.relerrdiff_LO < 0].sort_values(\"relerrdiff_LO\")[\n",
    "    ['relerrdiff_LO', 'LLVM-MCA_prediction', 'OSACA_throughput', 'OSACA_lcd', 'OSACA_cp',\n",
    "     'best_runtime', 'best_length', 'pointer_increment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "df_idx.query(\"kernel =='pi' and arch == 'TX2'\")[['OSACA_relerr', 'LLVM-MCA_relerr', 'best_runtime', 'pointer_increment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx = ('A64FX','clang', 'O2','update')\n",
    "r = df_idx.loc[idx]\n",
    "print(\"High-level iterations in assembly block:\", r.pointer_increment//8)\n",
    "print(\"Measured:\", r.best_runtime)\n",
    "for m in models:\n",
    "    print(m, \"Predicted:\", r[m+'_prediction'], \"TP:\", r[m+'_throughput'],\n",
    "          \"LCD:\", r[m+'_lcd'], \"CP:\", r[m+'_cp'])\n",
    "boxprint(r.OSACA_raw['output'])\n",
    "boxprint(r['LLVM-MCA_raw']['output'])\n",
    "boxprint(r['IACA_raw']['output'])\n",
    "#with open('/'.join(['build', *idx])+'.marked.s') as f:\n",
    "#    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "for a in archs:\n",
    "    if a == \"ZEN2\":\n",
    "        ylim = (-5,35)\n",
    "    else:\n",
    "        ylim = None\n",
    "    df[df.arch == a].sort_values('best_runtime', ignore_index=True).plot(\n",
    "        y=[m+'_prediction' for m in models]+['best_runtime', 'worst_runtime'],\n",
    "        style=['x', '.', '+', '-'], figsize=(20,5), xticks=[], title=a, ylim=ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for a in archs:\n",
    "    if a == \"ZEN2\":\n",
    "        ylim = (-5,35)\n",
    "    else:\n",
    "        ylim = None\n",
    "    df[df.arch == a].sort_values('best_runtime', ignore_index=True).plot(\n",
    "        y=['OSACA_throughput', 'OSACA_lcd', 'OSACA_cp', 'best_runtime', 'worst_runtime'],\n",
    "        style=['x', '.', '+', '-'], figsize=(20,5), ylim=ylim, xticks=[], title=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "for a in archs:\n",
    "    if a == \"ZEN2\":\n",
    "        ylim = (-5,35)\n",
    "    else:\n",
    "        ylim = None\n",
    "    df[df.arch == a].sort_values('best_runtime', ignore_index=True).plot(\n",
    "        y=['LLVM-MCA_throughput', 'LLVM-MCA_lcd', 'LLVM-MCA_cp', 'best_runtime', 'worst_runtime'],\n",
    "        style=['x', '.', '+', '-'], figsize=(20,5), ylim=ylim, xticks=[], title=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('python37': virtualenv)",
   "language": "python",
   "name": "python37564bitpython37virtualenve45cc6d57f8c49279c8350752a82a235"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
